{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "56fd32ff",
   "metadata": {},
   "source": [
    "\n",
    "# AI Interviewer — Conversation Flow Manager (LangGraph MVP)\n",
    "This notebook demonstrates a prototype conversation flow manager for an adaptive AI interviewer built using LangGraph. The system dynamically adjusts its questioning strategy based on a participant’s responses to verify their claimed skills efficiently and accurately.\n",
    "\n",
    "At its core, the agent maintains a per-skill belief state — a running mean and confidence band - to represent how confident it is that the respondent truly possesses each skill. Each turn of the conversation updates these beliefs and influences what the next question should be. The goal is to maximise verification accuracy while minimising interview time.\n",
    "\n",
    "## Objective\n",
    "Design a conversation manager that can:\n",
    "1.\tMeasure skill proficiency adaptively – gauge how well the respondent actually understands a claimed skill.\n",
    "2.\tBalance exploration and exploitation – explore unverified skills while deepening assessment of skills with partial evidence.\n",
    "3.\tOptimise for total verification reward – confirm as many skills as possible with strong evidence in limited turns.\n",
    "4.\tMinimise interview cost – reduce redundant or low-information questions to keep the process short and scalable. \n",
    "\n",
    "## System Overview\n",
    "The system operates as an information-seeking loop:\n",
    "\n",
    "Ask -> Evaluate -> Update Belief -> Choose Next Question \n",
    "\n",
    "Under the hood, it's implemented as a LangGraph state graph with a few key nodes: \n",
    "\n",
    "generate_candidates → select_question → ask → grade → update → decide\n",
    "\n",
    "\n",
    "1. **Generate Questions**: Propose a set of questions to ask the respondent. \n",
    "2. **Select Question**: Choose the most informative question from the set. \n",
    "3. **Ask**: Collect a response from the respondent. \n",
    "4. **Evaluate**: Grade the response based on a predefined rubric. \n",
    "5. **Update Belief**: Update the belief state based on the response. \n",
    "6. **Decide**: Determine what to do next based on the belief state. \n",
    "\n",
    "At the end, the system outputs Verification Cards summarising each skill’s confidence, lower-bound certainty, and verification status.\n",
    "\n",
    "## Design Reasoning \n",
    "1. Problem challenge: \n",
    "Traditional interview scripts are static and waste turns asking low-value or redundant questions. If we also do exhaustive probing of each skill, we may need 20+ turns. \n",
    "We need an interviewer that learns during the interaction. \n",
    "\n",
    "2. Key insight: \n",
    "The interview can be framed as an active learning or adaptive testing process — each question acts as an experiment that reduces uncertainty about the candidate’s expertise.\n",
    "\n",
    "3. Belif + policy:\n",
    "Maintain a running estimate of each skill’s confidence and pick the next question that yields the highest expected information gain (uncertainty reduction) within the turn budget. Compare to a brute-force Large Language Model (LLM) approach that simply generates questions based on a skill, this approach can learns and adapts within the interview, whilst still being able to verify the skills after the interview. \n",
    "\n",
    "4. Stopping criteration: \n",
    "Stop when further questioning provides minimal confidence improvement — efficient, not exhaustive.\n",
    "\n",
    "5.\tProof-of-concept scope:\n",
    "Domain: 3 claimed skills (e.g., PyTorch, LLM Evaluation, CUDA).\n",
    "Interview length: 8–10 turns.\n",
    "Output: dynamic evolution of skill confidence + verification summary.\n",
    "6.\tRisks and mitigations:\n",
    "LLM grading bias: Use fixed rubric + schema validation.\n",
    "Overconfidence early: Enforce minimum turns before verification.\n",
    "Verbose or evasive answers: Penalise low specificity in scoring.\n",
    "JSON drift: Strict schema with fallback templates.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bc6ed99",
   "metadata": {},
   "source": [
    "Before we start, let's assume that we have reconciled the participant's CV, LinkedIn profile and Google Scholar profile into a single structured profile.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ba1ccdd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "SKILLS_PROFILE = {\n",
    "    \"ID\": \"1234567890\",\n",
    "    \"NAME\": \"Jason T\",\n",
    "    \"EMAIL\": \"jason.t@example.com\",\n",
    "    \"PHONE\": \"+1234567890\",\n",
    "    \"SKILLS\": [\n",
    "        {\n",
    "            \"taxonomy_id\": \"ML/Frameworks/PyTorch\",\n",
    "            \"raw_aliases\": [\"pytorch\", \"pytorch lightning\"],\n",
    "            \"experience_years_claimed\": 3,\n",
    "            \"last_used_year\": 2025,\n",
    "            \"evidence_sources\": [\n",
    "                {\n",
    "                    \"source\": \"cv\",\n",
    "                    \"span\": \"Led development of a custom PyTorch-based deep learning pipeline for real-time object detection, achieving 95% accuracy while reducing inference time by 40%. Implemented distributed training using PyTorch Lightning across multiple GPUs.\",\n",
    "                    \"confidence\": 0.86,\n",
    "                },\n",
    "                {\n",
    "                    \"source\": \"linkedin\",\n",
    "                    \"span\": \"Developed and optimized deep learning models using PyTorch for computer vision applications. Created reusable model architectures and training pipelines with PyTorch Lightning that reduced development time by 60%.\",\n",
    "                    \"confidence\": 0.74,\n",
    "                },\n",
    "            ],\n",
    "        },\n",
    "        {\n",
    "            \"taxonomy_id\": \"Data Science/Frameworks/Pandas\",\n",
    "            \"raw_aliases\": [\"pandas\"],\n",
    "            \"experience_years_claimed\": 2,\n",
    "            \"last_used_year\": 2024,\n",
    "            \"evidence_sources\": [\n",
    "                {\n",
    "                    \"source\": \"cv\",\n",
    "                    \"span\": \"Built data processing pipelines using Pandas to clean and analyze 100GB+ of customer transaction data. Optimized memory usage by 70% through efficient DataFrame operations and custom aggregation functions.\",\n",
    "                    \"confidence\": 0.92,\n",
    "                },\n",
    "                {\n",
    "                    \"source\": \"linkedin\",\n",
    "                    \"span\": \"Extensive experience with Pandas for data manipulation and analysis. Created automated reporting systems processing millions of rows of time-series data with custom rolling window calculations.\",\n",
    "                    \"confidence\": 0.88,\n",
    "                },\n",
    "            ],\n",
    "        },\n",
    "    ],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1471b6cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict, List, Dict\n",
    "from enum import Enum\n",
    "\n",
    "skills_status = Enum(\"skills_status\", [\"unverified\", \"probed\", \"verified\"])\n",
    "\n",
    "\n",
    "# --- 1. Define the State for the Graph ---\n",
    "class InterviewState(TypedDict):\n",
    "    \"\"\"Represents the state of the interview at any given time.\"\"\"\n",
    "\n",
    "    # Inputs - Interview Data\n",
    "    skills_taxonomy: List[str]\n",
    "    skills_evidence_spans: Dict[str, List[str]]\n",
    "    skills_profile: Dict | None\n",
    "\n",
    "    # Conversation State\n",
    "    conversation_history: List[str]  # The history of the conversation\n",
    "    candidate_questions: List[\"Question\"]  # A pool of questions to choose from\n",
    "    current_question: str  # The current question\n",
    "    skills_status: Dict[str, skills_status]  # The status of the skills\n",
    "\n",
    "    # Processing State\n",
    "    turn_count: int\n",
    "\n",
    "    # Verification State\n",
    "    final_report: str  # The final report of the interview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8693bab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 2. Define PyDantic Models for structured LLM Outputs\n",
    "\n",
    "\n",
    "class Question(BaseModel):\n",
    "    \"\"\"A question to ask the candidate about a specific skill.\"\"\"\n",
    "\n",
    "    skill: str = Field(description=\"The skill this question is designed to test.\")\n",
    "    text: str = Field(description=\"The text of the question.\")\n",
    "    difficulty: int = Field(\n",
    "        description=\"The difficulty of the question, from 1 (easy) to 5 (expert).\"\n",
    "    )\n",
    "\n",
    "\n",
    "class FollowupQuestion(BaseModel):\n",
    "    \"\"\"Adaptive follow-up question conditioned on the last grade.\"\"\"\n",
    "\n",
    "    skill: str\n",
    "    text: str\n",
    "    difficulty: int\n",
    "    rationale: str\n",
    "\n",
    "\n",
    "class Grade(BaseModel):\n",
    "    \"\"\"A grade for the candidate's response.\"\"\"\n",
    "\n",
    "    score: int = Field(\n",
    "        description=\"The score from 1 (poor) to 5 (excellent) based on the rubric.\"\n",
    "    )\n",
    "    reasoning: str = Field(description=\"A brief justification for the given score.\")\n",
    "    subscores: Dict[str, float] = Field(\n",
    "        default_factory=dict,\n",
    "        description=\"Subscores in [0,1]: explicitness, specificity, recency, competence, consistency\",\n",
    "    )\n",
    "    confidence: float | None = Field(\n",
    "        default=None, ge=0.0, le=1.0, description=\"Weighted confidence in [0,1]\"\n",
    "    )\n",
    "\n",
    "\n",
    "class LLMGrade(BaseModel):\n",
    "    \"\"\"Minimal schema for structured LLM output.\"\"\"\n",
    "\n",
    "    score: int\n",
    "    reasoning: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b462835f",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "OpenAI API key not found. Please set it in your .env file.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      2\u001b[39m load_dotenv()\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mOPENAI_API_KEY\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m os.environ:\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mOpenAI API key not found. Please set it in your .env file.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# --- 3. Initialize the LLM ---\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# We'll use this LLM for all nodes that require generation or grading.\u001b[39;00m\n\u001b[32m      8\u001b[39m llm = ChatOpenAI(model=\u001b[33m\"\u001b[39m\u001b[33mgpt-4o\u001b[39m\u001b[33m\"\u001b[39m, temperature=\u001b[32m0.7\u001b[39m)\n",
      "\u001b[31mValueError\u001b[39m: OpenAI API key not found. Please set it in your .env file."
     ]
    }
   ],
   "source": [
    "# --- 3. Define the Graph ---\n",
    "\n",
    "\n",
    "def initialize_state(state: InterviewState) -> InterviewState:\n",
    "    \"\"\"Initialise the state of the interview.\"\"\"\n",
    "    return state\n",
    "\n",
    "\n",
    "def generate_questions_node(state: InterviewState) -> InterviewState:\n",
    "    \"\"\"Generate a set of questions to ask the candidate.\"\"\"\n",
    "    return state\n",
    "\n",
    "\n",
    "def select_question_node(state: InterviewState) -> InterviewState:\n",
    "    \"\"\"Select the most informative question from the set.\"\"\"\n",
    "    return state\n",
    "\n",
    "\n",
    "def ask_question_node(state: InterviewState) -> InterviewState:\n",
    "    \"\"\"Ask the candidate a question.\"\"\"\n",
    "    return state\n",
    "\n",
    "\n",
    "def grade_response_node(state: InterviewState) -> InterviewState:\n",
    "    \"\"\"Grade the candidate's response.\"\"\"\n",
    "    return state\n",
    "\n",
    "\n",
    "def update_belief_node(state: InterviewState) -> InterviewState:\n",
    "    \"\"\"Update the belief state based on the response.\"\"\"\n",
    "    return state\n",
    "\n",
    "\n",
    "def generate_report_node(state: InterviewState) -> InterviewState:\n",
    "    \"\"\"Generate a report of the interview.\"\"\"\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b4dc501",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Edges\n",
    "def should_continue_node(state: InterviewState) -> InterviewState:\n",
    "    if state[\"turn_count\"] >= state[\"max_turns\"]:\n",
    "        return \"end\"\n",
    "    else:\n",
    "        return \"continue\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "597d4561",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assemble the graph\n",
    "workflow = StateGraph(InterviewState)\n",
    "workflow.add_node(\"initialize\", initialize_state)\n",
    "workflow.add_node(\"generate_questions\", generate_questions_node)\n",
    "workflow.add_node(\"select_question\", select_question_node)\n",
    "workflow.add_node(\"ask_question\", ask_question_node)\n",
    "workflow.add_node(\"grade_response\", grade_response_node)\n",
    "\n",
    "# Set entry point\n",
    "workflow.set_entry_point(\"initialize\")\n",
    "\n",
    "# Add edges\n",
    "workflow.add_edge(\"initialize\", \"generate_questions\")\n",
    "workflow.add_edge(\"generate_questions\", \"select_question\")\n",
    "workflow.add_edge(\"select_question\", \"ask_question\")\n",
    "workflow.add_edge(\"ask_question\", \"grade_response\")\n",
    "workflow.add_edge(\"grade_response\", \"update_belief\")\n",
    "workflow.add_edge(\"update_belief\", \"generate_questions\")\n",
    "workflow.add_edge(\"generate_questions\", \"should_continue\")\n",
    "\n",
    "\n",
    "# Add conditional edges\n",
    "workflow.add_conditional_edges(\n",
    "    \"should_continue\",\n",
    "    should_continue_node,\n",
    "    {\"continue\": \"generate_questions\", \"end\": \"generate_report\"},\n",
    ")\n",
    "\n",
    "workflow.add_edge(\"generate_report\", END)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
