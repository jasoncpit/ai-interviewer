{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "21a25771",
   "metadata": {},
   "source": [
    "\n",
    "# AI Interviewer — Conversation Flow Manager (LangGraph MVP)\n",
    "This notebook demonstrates a prototype conversation flow manager for an adaptive AI interviewer built using LangGraph. The system dynamically adjusts its questioning strategy based on a participant’s responses to verify their claimed skills efficiently and accurately.\n",
    "\n",
    "At its core, the agent maintains a per-skill belief state — a running mean and confidence band - to represent how confident it is that the respondent truly possesses each skill. Each turn of the conversation updates these beliefs and influences what the next question should be. The goal is to maximise verification accuracy while minimising interview time.\n",
    "\n",
    "## Objective\n",
    "Design a conversation manager that can:\n",
    "1.\tMeasure skill proficiency adaptively – gauge how well the respondent actually understands a claimed skill.\n",
    "2.\tBalance exploration and exploitation – explore unverified skills while deepening assessment of skills with partial evidence.\n",
    "3.\tOptimise for total verification reward – confirm as many skills as possible with strong evidence in limited turns.\n",
    "4.\tMinimise interview cost – reduce redundant or low-information questions to keep the process short and scalable. \n",
    "\n",
    "## System Overview\n",
    "The system operates as an information-seeking loop:\n",
    "\n",
    "Ask -> Evaluate -> Update Belief -> Choose Next Question \n",
    "\n",
    "Under the hood, it's implemented as a LangGraph state graph with a few key nodes: \n",
    "\n",
    "generate_candidates → select_question → ask → grade → update → decide\n",
    "\n",
    "\n",
    "1. **Generate Questions**: Propose a set of questions to ask the respondent. \n",
    "2. **Select Question**: Choose the most informative question from the set. \n",
    "3. **Ask**: Collect a response from the respondent. \n",
    "4. **Evaluate**: Grade the response based on a predefined rubric. \n",
    "5. **Update Belief**: Update the belief state based on the response. \n",
    "6. **Decide**: Determine what to do next based on the belief state. \n",
    "\n",
    "At the end, the system outputs Verification Cards summarising each skill’s confidence, lower-bound certainty, and verification status.\n",
    "\n",
    "## Design Reasoning \n",
    "1. Problem challenge: \n",
    "Traditional interview scripts are static and waste turns asking low-value or redundant questions. If we also do exhaustive probing of each skill, we may need 20+ turns. \n",
    "We need an interviewer that learns during the interaction. \n",
    "\n",
    "2. Key insight: \n",
    "The interview can be framed as an active learning or adaptive testing process — each question acts as an experiment that reduces uncertainty about the candidate’s expertise.\n",
    "\n",
    "3. Belif + policy:\n",
    "Maintain a running estimate of each skill’s confidence and pick the next question that yields the highest expected information gain (uncertainty reduction) within the turn budget. Compare to a brute-force Large Language Model (LLM) approach that simply generates questions based on a skill, this approach can learns and adapts within the interview, whilst still being able to verify the skills after the interview. \n",
    "\n",
    "4. Stopping criteration: \n",
    "Stop when further questioning provides minimal confidence improvement — efficient, not exhaustive.\n",
    "\n",
    "5.\tProof-of-concept scope:\n",
    "Domain: 3 claimed skills (e.g., PyTorch, LLM Evaluation, CUDA).\n",
    "Interview length: 8–10 turns.\n",
    "Output: dynamic evolution of skill confidence + verification summary.\n",
    "6.\tRisks and mitigations:\n",
    "LLM grading bias: Use fixed rubric + schema validation.\n",
    "Overconfidence early: Enforce minimum turns before verification.\n",
    "Verbose or evasive answers: Penalise low specificity in scoring.\n",
    "JSON drift: Strict schema with fallback templates.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6de52dac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jason_macstudio/.venv/lib/python3.11/site-packages/IPython/core/interactiveshell.py:3699: LangChainDeprecationWarning: As of langchain-core 0.3.0, LangChain uses pydantic v2 internally. The langchain_core.pydantic_v1 module was a compatibility shim for pydantic v1, and should no longer be used. Please update the code to import from Pydantic directly.\n",
      "\n",
      "For example, replace imports like: `from langchain_core.pydantic_v1 import BaseModel`\n",
      "with: `from pydantic import BaseModel`\n",
      "or the v1 compatibility namespace if you are working in a code base that has not been fully upgraded to pydantic 2 yet. \tfrom pydantic.v1 import BaseModel\n",
      "\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "/Users/jason_macstudio/.venv/lib/python3.11/site-packages/langchain_openai/chat_models/base.py:1936: UserWarning: Received a Pydantic BaseModel V1 schema. This is not supported by method=\"json_schema\". Please use method=\"function_calling\" or specify schema via JSON Schema or Pydantic V2 BaseModel. Overriding to method=\"function_calling\".\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System: Initializing interview for skills: PyTorch, LLM Evaluation, CUDA\n",
      "--------------------------------------------------\n",
      "System: Generating candidate questions...\n",
      "System: Selecting best question using UCB policy...\n",
      "  - Skill: PyTorch, Mean: 0.00, N: 0, UCB Score: inf\n",
      "  - Skill: LLM Evaluation, Mean: 0.00, N: 0, UCB Score: inf\n",
      "  - Skill: CUDA, Mean: 0.00, N: 0, UCB Score: inf\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'candidate_response'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[47]\u001b[39m\u001b[32m, line 325\u001b[39m\n\u001b[32m    319\u001b[39m initial_state: InterviewState = {\n\u001b[32m    320\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mskills_to_verify\u001b[39m\u001b[33m\"\u001b[39m: initial_skills,\n\u001b[32m    321\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mmax_turns\u001b[39m\u001b[33m\"\u001b[39m: max_interview_turns,\n\u001b[32m    322\u001b[39m }\n\u001b[32m    324\u001b[39m \u001b[38;5;66;03m# Stream the graph execution\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m325\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mevent\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mapp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m(\u001b[49m\u001b[43minitial_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    326\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# The stream method yields a dictionary for each step that has finished\u001b[39;49;00m\n\u001b[32m    327\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# The key is the node name and the value is the output of that node\u001b[39;49;00m\n\u001b[32m    328\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnode_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate_update\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43miter\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mevent\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    330\u001b[39m     \u001b[38;5;66;03m# The graph pauses implicitly when it needs input that isn't available.\u001b[39;00m\n\u001b[32m    331\u001b[39m     \u001b[38;5;66;03m# We design our graph so the 'select_question' node prepares the question.\u001b[39;00m\n\u001b[32m    332\u001b[39m     \u001b[38;5;66;03m# After 'select_question' runs, we intercept, ask the user, and then reinvoke.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    340\u001b[39m \n\u001b[32m    341\u001b[39m \u001b[38;5;66;03m# A more script-friendly execution flow\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.venv/lib/python3.11/site-packages/langgraph/pregel/main.py:2674\u001b[39m, in \u001b[36mPregel.stream\u001b[39m\u001b[34m(self, input, config, context, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, durability, subgraphs, debug, **kwargs)\u001b[39m\n\u001b[32m   2672\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m task \u001b[38;5;129;01min\u001b[39;00m loop.match_cached_writes():\n\u001b[32m   2673\u001b[39m     loop.output_writes(task.id, task.writes, cached=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m-> \u001b[39m\u001b[32m2674\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrunner\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtick\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2675\u001b[39m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mloop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtasks\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwrites\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2676\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstep_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2677\u001b[39m \u001b[43m    \u001b[49m\u001b[43mget_waiter\u001b[49m\u001b[43m=\u001b[49m\u001b[43mget_waiter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2678\u001b[39m \u001b[43m    \u001b[49m\u001b[43mschedule_task\u001b[49m\u001b[43m=\u001b[49m\u001b[43mloop\u001b[49m\u001b[43m.\u001b[49m\u001b[43maccept_push\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2679\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   2680\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# emit output\u001b[39;49;00m\n\u001b[32m   2681\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01myield from\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_output\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2682\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprint_mode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msubgraphs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mqueue\u001b[49m\u001b[43m.\u001b[49m\u001b[43mEmpty\u001b[49m\n\u001b[32m   2683\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2684\u001b[39m loop.after_tick()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.venv/lib/python3.11/site-packages/langgraph/pregel/_runner.py:162\u001b[39m, in \u001b[36mPregelRunner.tick\u001b[39m\u001b[34m(self, tasks, reraise, timeout, retry_policy, get_waiter, schedule_task)\u001b[39m\n\u001b[32m    160\u001b[39m t = tasks[\u001b[32m0\u001b[39m]\n\u001b[32m    161\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m162\u001b[39m     \u001b[43mrun_with_retry\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    163\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    164\u001b[39m \u001b[43m        \u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    165\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconfigurable\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m    166\u001b[39m \u001b[43m            \u001b[49m\u001b[43mCONFIG_KEY_CALL\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpartial\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    167\u001b[39m \u001b[43m                \u001b[49m\u001b[43m_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    168\u001b[39m \u001b[43m                \u001b[49m\u001b[43mweakref\u001b[49m\u001b[43m.\u001b[49m\u001b[43mref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    169\u001b[39m \u001b[43m                \u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    170\u001b[39m \u001b[43m                \u001b[49m\u001b[43mfutures\u001b[49m\u001b[43m=\u001b[49m\u001b[43mweakref\u001b[49m\u001b[43m.\u001b[49m\u001b[43mref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfutures\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    171\u001b[39m \u001b[43m                \u001b[49m\u001b[43mschedule_task\u001b[49m\u001b[43m=\u001b[49m\u001b[43mschedule_task\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    172\u001b[39m \u001b[43m                \u001b[49m\u001b[43msubmit\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msubmit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    173\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    174\u001b[39m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    175\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    176\u001b[39m     \u001b[38;5;28mself\u001b[39m.commit(t, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    177\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.venv/lib/python3.11/site-packages/langgraph/pregel/_retry.py:42\u001b[39m, in \u001b[36mrun_with_retry\u001b[39m\u001b[34m(task, retry_policy, configurable)\u001b[39m\n\u001b[32m     40\u001b[39m     task.writes.clear()\n\u001b[32m     41\u001b[39m     \u001b[38;5;66;03m# run the task\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m42\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtask\u001b[49m\u001b[43m.\u001b[49m\u001b[43mproc\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m.\u001b[49m\u001b[43minput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     43\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ParentCommand \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m     44\u001b[39m     ns: \u001b[38;5;28mstr\u001b[39m = config[CONF][CONFIG_KEY_CHECKPOINT_NS]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.venv/lib/python3.11/site-packages/langgraph/_internal/_runnable.py:657\u001b[39m, in \u001b[36mRunnableSeq.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    655\u001b[39m     \u001b[38;5;66;03m# run in context\u001b[39;00m\n\u001b[32m    656\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m set_config_context(config, run) \u001b[38;5;28;01mas\u001b[39;00m context:\n\u001b[32m--> \u001b[39m\u001b[32m657\u001b[39m         \u001b[38;5;28minput\u001b[39m = \u001b[43mcontext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    658\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    659\u001b[39m     \u001b[38;5;28minput\u001b[39m = step.invoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.venv/lib/python3.11/site-packages/langgraph/_internal/_runnable.py:401\u001b[39m, in \u001b[36mRunnableCallable.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    399\u001b[39m         run_manager.on_chain_end(ret)\n\u001b[32m    400\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m401\u001b[39m     ret = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    402\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.recurse \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ret, Runnable):\n\u001b[32m    403\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m ret.invoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[47]\u001b[39m\u001b[32m, line 169\u001b[39m, in \u001b[36mgrade_response_node\u001b[39m\u001b[34m(state)\u001b[39m\n\u001b[32m    164\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    165\u001b[39m \u001b[33;03mNode 4: Grade the candidate's response based on a rubric.\u001b[39;00m\n\u001b[32m    166\u001b[39m \u001b[33;03mThis corresponds to your \"Evaluate\" step.\u001b[39;00m\n\u001b[32m    167\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    168\u001b[39m question = state[\u001b[33m\"\u001b[39m\u001b[33mcurrent_question\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m--> \u001b[39m\u001b[32m169\u001b[39m response = \u001b[43mstate\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcandidate_response\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m    171\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mSystem: Grading response...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    173\u001b[39m prompt = ChatPromptTemplate.from_template(\u001b[33m\"\"\"\u001b[39m\n\u001b[32m    174\u001b[39m \u001b[33mYou are an expert technical interviewer grading a candidate\u001b[39m\u001b[33m'\u001b[39m\u001b[33ms response.\u001b[39m\n\u001b[32m    175\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    189\u001b[39m \u001b[33mBased on the rubric, provide a score and a brief reasoning. Penalize evasive or low-specificity answers.\u001b[39m\n\u001b[32m    190\u001b[39m \u001b[33m\u001b[39m\u001b[33m\"\"\"\u001b[39m)\n",
      "\u001b[31mKeyError\u001b[39m: 'candidate_response'",
      "During task with name 'grade_response' and id '6a63bcb6-8374-902c-17bf-ed6849e02087'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "from typing import TypedDict, List, Dict\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langgraph.graph import StateGraph, END\n",
    "\n",
    "# --- Load API Key ---\n",
    "load_dotenv()\n",
    "if \"OPENAI_API_KEY\" not in os.environ:\n",
    "    raise ValueError(\"OpenAI API key not found. Please set it in your .env file.\")\n",
    "\n",
    "\n",
    "# --- 1. Define the State for the Graph ---\n",
    "# This is the central object that flows through the system, tracking everything.\n",
    "class InterviewState(TypedDict):\n",
    "    \"\"\"Represents the state of the interview at any given time.\"\"\"\n",
    "\n",
    "    skills_to_verify: List[str]\n",
    "    belief_state: Dict[\n",
    "        str, Dict\n",
    "    ]  # Maps skill -> {'mean_score': float, 'n_questions': int}\n",
    "    conversation_history: List[str]\n",
    "    candidate_questions: List[\"Question\"]  # A pool of questions to choose from\n",
    "    current_question: \"Question\"\n",
    "    candidate_response: str\n",
    "    grade: \"Grade\"\n",
    "    turn_count: int\n",
    "    max_turns: int\n",
    "    final_report: str\n",
    "\n",
    "\n",
    "# --- 2. Define Pydantic Models for Structured LLM Outputs ---\n",
    "# These ensure the LLM provides responses in a reliable, parsable format.\n",
    "\n",
    "\n",
    "class Question(BaseModel):\n",
    "    \"\"\"A question to ask the candidate about a specific skill.\"\"\"\n",
    "\n",
    "    skill: str = Field(description=\"The skill this question is designed to test.\")\n",
    "    text: str = Field(description=\"The text of the question.\")\n",
    "    difficulty: int = Field(\n",
    "        description=\"The difficulty of the question, from 1 (easy) to 5 (expert).\"\n",
    "    )\n",
    "\n",
    "\n",
    "class Grade(BaseModel):\n",
    "    \"\"\"A grade for the candidate's response.\"\"\"\n",
    "\n",
    "    score: int = Field(\n",
    "        description=\"The score from 1 (poor) to 5 (excellent) based on the rubric.\"\n",
    "    )\n",
    "    reasoning: str = Field(description=\"A brief justification for the given score.\")\n",
    "\n",
    "\n",
    "# --- 3. Initialize the LLM ---\n",
    "# We'll use this LLM for all nodes that require generation or grading.\n",
    "llm = ChatOpenAI(model=\"gpt-4o\", temperature=0.7)\n",
    "\n",
    "# --- 4. Define the Nodes of the Graph ---\n",
    "# Each function represents a step in your conversation flow.\n",
    "\n",
    "\n",
    "def initialize_state(state: InterviewState) -> InterviewState:\n",
    "    \"\"\"Initializes the belief state for all skills.\"\"\"\n",
    "    belief_state = {\n",
    "        skill: {\"mean_score\": 0.0, \"n_questions\": 0}\n",
    "        for skill in state[\"skills_to_verify\"]\n",
    "    }\n",
    "    state[\"belief_state\"] = belief_state\n",
    "    state[\"turn_count\"] = 0\n",
    "    state[\"conversation_history\"] = []\n",
    "    print(\n",
    "        \"System: Initializing interview for skills:\",\n",
    "        \", \".join(state[\"skills_to_verify\"]),\n",
    "    )\n",
    "    print(\"-\" * 50)\n",
    "    return state\n",
    "\n",
    "\n",
    "def generate_questions_node(state: InterviewState) -> InterviewState:\n",
    "    \"\"\"\n",
    "    Node 1: Generate a set of candidate questions, one for each skill.\n",
    "    This corresponds to your \"Generate Questions\" step.\n",
    "    \"\"\"\n",
    "    skills = state[\"skills_to_verify\"]\n",
    "    belief_state = state[\"belief_state\"]\n",
    "    history = \"\\n\".join(state[\"conversation_history\"])\n",
    "\n",
    "    print(\"System: Generating candidate questions...\")\n",
    "\n",
    "    prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\n",
    "                \"system\",\n",
    "                \"\"\"You are an expert technical interviewer. Your task is to generate one insightful question for the given skill.\n",
    "        The question should be designed to verify the candidate's true understanding.\n",
    "        Based on the candidate's average score for this skill so far, adjust the difficulty:\n",
    "        - If the score is low (0-2), ask an easy/foundational question.\n",
    "        - If the score is medium (2-4), ask an intermediate/practical question.\n",
    "        - If the score is high (4+), ask a difficult/nuanced question.\n",
    "        Avoid repeating questions from the conversation history.\"\"\",\n",
    "            ),\n",
    "            (\n",
    "                \"human\",\n",
    "                \"\"\"\n",
    "        Skill: {skill}\n",
    "        Current Average Score for this skill: {avg_score:.2f}\n",
    "        Conversation History:\n",
    "        {history}\n",
    "\n",
    "        Generate one question about {skill}.\n",
    "        \"\"\",\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    structured_llm = llm.with_structured_output(Question)\n",
    "    question_prompts = [\n",
    "        prompt.format_prompt(\n",
    "            skill=skill, avg_score=belief_state[skill][\"mean_score\"], history=history\n",
    "        )\n",
    "        for skill in skills\n",
    "    ]\n",
    "\n",
    "    candidate_questions = structured_llm.batch(question_prompts)\n",
    "    state[\"candidate_questions\"] = candidate_questions\n",
    "    return state\n",
    "\n",
    "\n",
    "def select_question_node(state: InterviewState) -> InterviewState:\n",
    "    \"\"\"\n",
    "    Node 2: Select the most informative question using the UCB1 algorithm.\n",
    "    This corresponds to your \"Select Question\" step and implements the exploration/exploitation policy.\n",
    "    \"\"\"\n",
    "    candidate_questions = state[\"candidate_questions\"]\n",
    "    belief_state = state[\"belief_state\"]\n",
    "    total_turns = state[\"turn_count\"]\n",
    "    exploration_factor = 2.0  # The 'C' constant in the UCB formula\n",
    "\n",
    "    best_question = None\n",
    "    max_ucb_score = -1\n",
    "\n",
    "    print(\"System: Selecting best question using UCB policy...\")\n",
    "\n",
    "    for question in candidate_questions:\n",
    "        skill = question.skill\n",
    "        n_s = belief_state[skill][\"n_questions\"]\n",
    "        mean_s = belief_state[skill][\"mean_score\"]\n",
    "\n",
    "        if n_s == 0:\n",
    "            # If a skill has never been asked about, its priority is infinite.\n",
    "            ucb_score = float(\"inf\")\n",
    "        else:\n",
    "            # UCB1 Formula\n",
    "            ucb_score = mean_s + exploration_factor * math.sqrt(\n",
    "                math.log(total_turns + 1) / n_s\n",
    "            )\n",
    "\n",
    "        print(\n",
    "            f\"  - Skill: {skill}, Mean: {mean_s:.2f}, N: {n_s}, UCB Score: {ucb_score:.2f}\"\n",
    "        )\n",
    "\n",
    "        if ucb_score > max_ucb_score:\n",
    "            max_ucb_score = ucb_score\n",
    "            best_question = question\n",
    "\n",
    "    state[\"current_question\"] = best_question\n",
    "    return state\n",
    "\n",
    "\n",
    "def ask_question_and_get_response(state: InterviewState) -> InterviewState:\n",
    "    \"\"\"\n",
    "    Node 3: Ask the selected question and get a response from the user.\n",
    "    This is the human-in-the-loop part of the graph.\n",
    "    \"\"\"\n",
    "    question = state[\"current_question\"]\n",
    "\n",
    "    # Append AI's turn to history\n",
    "    ai_message = f\"AI Interviewer: {question.text}\"\n",
    "    state[\"conversation_history\"].append(ai_message)\n",
    "    print(\"\\n\" + ai_message)\n",
    "\n",
    "    # Get human response\n",
    "    human_response = input(\"Your Answer: \")\n",
    "    state[\"candidate_response\"] = human_response\n",
    "\n",
    "    # Append human's turn to history\n",
    "    human_message = f\"Candidate: {human_response}\"\n",
    "    state[\"conversation_history\"].append(human_message)\n",
    "\n",
    "    return state\n",
    "\n",
    "\n",
    "def grade_response_node(state: InterviewState) -> InterviewState:\n",
    "    \"\"\"\n",
    "    Node 4: Grade the candidate's response based on a rubric.\n",
    "    This corresponds to your \"Evaluate\" step.\n",
    "    \"\"\"\n",
    "    question = state[\"current_question\"]\n",
    "    response = state[\"candidate_response\"]\n",
    "\n",
    "    print(\"System: Grading response...\")\n",
    "\n",
    "    prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "    You are an expert technical interviewer grading a candidate's response.\n",
    "    \n",
    "    **Rubric:**\n",
    "    - 1: Completely incorrect or irrelevant.\n",
    "    - 2: Shows minimal understanding, contains major errors.\n",
    "    - 3: Partially correct, but misses key details or contains inaccuracies.\n",
    "    - 4: Mostly correct and well-explained, but could be more detailed or nuanced.\n",
    "    - 5: Excellent. A thorough, accurate, and insightful answer, demonstrating deep understanding.\n",
    "\n",
    "    **Question Asked:**\n",
    "    '{question_text}'\n",
    "\n",
    "    **Candidate's Response:**\n",
    "    '{candidate_response}'\n",
    "\n",
    "    Based on the rubric, provide a score and a brief reasoning. Penalize evasive or low-specificity answers.\n",
    "    \"\"\")\n",
    "\n",
    "    grading_llm = llm.with_structured_output(Grade)\n",
    "    grade = grading_llm.invoke(\n",
    "        prompt.format(question_text=question.text, candidate_response=response)\n",
    "    )\n",
    "\n",
    "    state[\"grade\"] = grade\n",
    "    print(f\"System: Grade assigned: {grade.score}/5. Reason: {grade.reasoning}\")\n",
    "    return state\n",
    "\n",
    "\n",
    "def update_belief_node(state: InterviewState) -> InterviewState:\n",
    "    \"\"\"\n",
    "    Node 5: Update the belief state with the new grade.\n",
    "    This corresponds to your \"Update Belief\" step.\n",
    "    \"\"\"\n",
    "    grade = state[\"grade\"]\n",
    "    skill_in_focus = state[\"current_question\"].skill\n",
    "\n",
    "    # Update belief state for the questioned skill\n",
    "    current_belief = state[\"belief_state\"][skill_in_focus]\n",
    "    old_mean = current_belief[\"mean_score\"]\n",
    "    n = current_belief[\"n_questions\"]\n",
    "\n",
    "    # Running mean calculation\n",
    "    new_mean = (old_mean * n + grade.score) / (n + 1)\n",
    "\n",
    "    state[\"belief_state\"][skill_in_focus][\"mean_score\"] = new_mean\n",
    "    state[\"belief_state\"][skill_in_focus][\"n_questions\"] += 1\n",
    "\n",
    "    # Increment global turn count\n",
    "    state[\"turn_count\"] += 1\n",
    "\n",
    "    print(\"System: Belief state updated.\")\n",
    "    print(\"Current Beliefs:\")\n",
    "    for skill, belief in state[\"belief_state\"].items():\n",
    "        print(\n",
    "            f\"  - {skill}: Mean Score = {belief['mean_score']:.2f}, Questions Asked = {belief['n_questions']}\"\n",
    "        )\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "    return state\n",
    "\n",
    "\n",
    "def generate_report_node(state: InterviewState) -> InterviewState:\n",
    "    \"\"\"\n",
    "    Final Node: Generate a summary report of the interview.\n",
    "    This creates the \"Verification Cards\" you described.\n",
    "    \"\"\"\n",
    "    belief_state = state[\"belief_state\"]\n",
    "    report_parts = [\"# AI Interviewer: Final Verification Report\\n\"]\n",
    "\n",
    "    for skill, belief in belief_state.items():\n",
    "        mean_score = belief[\"mean_score\"]\n",
    "        n_questions = belief[\"n_questions\"]\n",
    "\n",
    "        status = \"Insufficient Data\"\n",
    "        if n_questions >= 2:  # Min turns before verification (as per your plan)\n",
    "            if mean_score >= 3.75:\n",
    "                status = \"✅ Verified\"\n",
    "            else:\n",
    "                status = \"❌ Not Verified\"\n",
    "\n",
    "        card = f\"\"\"\n",
    "        ## Skill: {skill}\n",
    "        - **Verification Status**: {status}\n",
    "        - **Confidence Score**: {mean_score:.2f} / 5.0\n",
    "        - **Evidence Strength**: {n_questions} questions asked.\n",
    "        \"\"\"\n",
    "        report_parts.append(card)\n",
    "\n",
    "    final_report = \"\\n\".join(report_parts)\n",
    "    state[\"final_report\"] = final_report\n",
    "    return state\n",
    "\n",
    "\n",
    "# --- 5. Define Conditional Logic ---\n",
    "def should_continue(state: InterviewState) -> str:\n",
    "    \"\"\"\n",
    "    Node 6: Decide whether to continue the interview or end it.\n",
    "    This is the conditional edge logic.\n",
    "    \"\"\"\n",
    "    if state[\"turn_count\"] >= state[\"max_turns\"]:\n",
    "        print(\"System: Maximum turns reached. Concluding interview.\")\n",
    "        return \"end\"\n",
    "    else:\n",
    "        return \"continue\"\n",
    "\n",
    "\n",
    "# --- 6. Assemble the Graph ---\n",
    "workflow = StateGraph(InterviewState)\n",
    "\n",
    "# Add nodes to the graph\n",
    "workflow.add_node(\"initialize_state\", initialize_state)\n",
    "workflow.add_node(\"generate_questions\", generate_questions_node)\n",
    "workflow.add_node(\"select_question\", select_question_node)\n",
    "workflow.add_node(\n",
    "    \"ask_and_get_response\", ask_question_and_get_response\n",
    ")  # This will be handled in the loop\n",
    "workflow.add_node(\"grade_response\", grade_response_node)\n",
    "workflow.add_node(\"update_belief\", update_belief_node)\n",
    "workflow.add_node(\"generate_report\", generate_report_node)\n",
    "\n",
    "# Set the entry point\n",
    "workflow.set_entry_point(\"initialize_state\")\n",
    "\n",
    "# Add edges to define the flow\n",
    "workflow.add_edge(\"initialize_state\", \"generate_questions\")\n",
    "workflow.add_edge(\"generate_questions\", \"select_question\")\n",
    "# The \"ask\" step is where we'll pause, so no direct edge in the same way\n",
    "workflow.add_edge(\n",
    "    \"select_question\", \"grade_response\"\n",
    ")  # This is a conceptual link; we need to insert the human step\n",
    "workflow.add_edge(\"grade_response\", \"update_belief\")\n",
    "\n",
    "# Add the conditional edge\n",
    "workflow.add_conditional_edges(\n",
    "    \"update_belief\",\n",
    "    should_continue,\n",
    "    {\n",
    "        \"continue\": \"generate_questions\",\n",
    "        \"end\": \"generate_report\",\n",
    "    },\n",
    ")\n",
    "\n",
    "workflow.add_edge(\"generate_report\", END)\n",
    "\n",
    "# Compile the graph\n",
    "app = workflow.compile()\n",
    "\n",
    "\n",
    "# --- 7. Run the Interview ---\n",
    "if __name__ == \"__main__\":\n",
    "    # Define the interview parameters (as per your PoC scope)\n",
    "    initial_skills = [\"PyTorch\", \"LLM Evaluation\", \"CUDA\"]\n",
    "    max_interview_turns = 8\n",
    "\n",
    "    # Initial state\n",
    "    config = {\"recursion_limit\": 100}\n",
    "    initial_state: InterviewState = {\n",
    "        \"skills_to_verify\": initial_skills,\n",
    "        \"max_turns\": max_interview_turns,\n",
    "    }\n",
    "\n",
    "    # Stream the graph execution\n",
    "    for event in app.stream(initial_state, config=config):\n",
    "        # The stream method yields a dictionary for each step that has finished\n",
    "        # The key is the node name and the value is the output of that node\n",
    "        node_name, state_update = next(iter(event.items()))\n",
    "\n",
    "        # The graph pauses implicitly when it needs input that isn't available.\n",
    "        # We design our graph so the 'select_question' node prepares the question.\n",
    "        # After 'select_question' runs, we intercept, ask the user, and then reinvoke.\n",
    "        # However, a simpler way for a script is to just manage the human-in-the-loop\n",
    "        # part manually by modifying state.\n",
    "\n",
    "        # A clean way to handle the human step:\n",
    "        # A proper LangGraph pattern would be to have a separate node for the human input,\n",
    "        # and compile the graph with an interruption. For this script, we can simulate it.\n",
    "        # Let's rebuild the flow to be more interactive-script friendly.\n",
    "\n",
    "    # A more script-friendly execution flow\n",
    "    print(\"--- Starting AI Interview ---\")\n",
    "\n",
    "    current_state = initialize_state(initial_state)\n",
    "\n",
    "    while current_state[\"turn_count\"] < current_state[\"max_turns\"]:\n",
    "        print(\n",
    "            f\"\\n--- Turn {current_state['turn_count'] + 1}/{current_state['max_turns']} ---\"\n",
    "        )\n",
    "        current_state = generate_questions_node(current_state)\n",
    "        current_state = select_question_node(current_state)\n",
    "        current_state = ask_question_and_get_response(current_state)\n",
    "        current_state = grade_response_node(current_state)\n",
    "        current_state = update_belief_node(current_state)\n",
    "\n",
    "    print(\"\\n--- Interview Finished ---\")\n",
    "    final_state = generate_report_node(current_state)\n",
    "    print(final_state[\"final_report\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a458bb1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Toggle MOCK vs REAL ---\n",
    "MOCK = (\n",
    "    False  # Set to False to use real LLM + LangGraph (requires installs and API keys)\n",
    ")\n",
    "\n",
    "# Optional: your OpenAI key if MOCK=False\n",
    "import os\n",
    "\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\", \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ad0652a",
   "metadata": {},
   "source": [
    "\n",
    "## 0) Setup (only needed for REAL mode)\n",
    "\n",
    "If you plan to run with a real LLM and LangGraph, install:\n",
    "```bash\n",
    "pip install langgraph langchain openai pydantic\n",
    "```\n",
    "Set your environment variable:\n",
    "```bash\n",
    "export OPENAI_API_KEY=sk-...\n",
    "```\n",
    "Then set `MOCK=False` in the cell above.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a21b813f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List, Optional, Literal, TypedDict\n",
    "import random\n",
    "import json\n",
    "\n",
    "# Try importing LangGraph only when needed\n",
    "if not MOCK:\n",
    "    from langgraph.graph import StateGraph, END\n",
    "    from langgraph.checkpoint.memory import MemorySaver\n",
    "else:\n",
    "    # Light shims so the rest of the notebook runs in MOCK mode without LangGraph\n",
    "    class StateGraph:\n",
    "        def __init__(self, state_type=None):\n",
    "            self.nodes = {}\n",
    "            self.entry = None\n",
    "            self.conditional_edges = {}\n",
    "            self.edges = []\n",
    "\n",
    "        def add_node(self, name, fn):\n",
    "            self.nodes[name] = fn\n",
    "\n",
    "        def set_entry_point(self, name):\n",
    "            self.entry = name\n",
    "\n",
    "        def add_edge(self, a, b):\n",
    "            self.edges.append((a, b))\n",
    "\n",
    "        def add_conditional_edges(self, node, fn, mapping):\n",
    "            self.conditional_edges[node] = (fn, mapping)\n",
    "\n",
    "        def compile(self, checkpointer=None):\n",
    "            return self\n",
    "\n",
    "        def invoke(self, state):\n",
    "            # A tiny runner that follows the fixed path init→... with a single loop\n",
    "            current = self.entry\n",
    "            while True:\n",
    "                state = self.nodes[current](state)\n",
    "                # Decide next hop\n",
    "                # 1) If conditional edges from current\n",
    "                if current in self.conditional_edges:\n",
    "                    fn, mapping = self.conditional_edges[current]\n",
    "                    decision = fn(state)\n",
    "                    nxt = mapping[decision]\n",
    "                else:\n",
    "                    # else find the first matching edge\n",
    "                    nexts = [b for (a, b) in self.edges if a == current]\n",
    "                    nxt = nexts[0] if nexts else None\n",
    "                if not nxt:\n",
    "                    break\n",
    "                if nxt == \"END\":\n",
    "                    break\n",
    "                current = nxt\n",
    "            return state\n",
    "\n",
    "    class MemorySaver:\n",
    "        pass\n",
    "\n",
    "\n",
    "# Optional: real LLM clients when MOCK=False\n",
    "if not MOCK:\n",
    "    from langchain_openai import ChatOpenAI\n",
    "\n",
    "    llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0, api_key=OPENAI_API_KEY)\n",
    "else:\n",
    "    llm = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fbce495e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Status = Literal[\"unverified\", \"probed\", \"verified\"]\n",
    "\n",
    "\n",
    "class EvalJSON(TypedDict):\n",
    "    correctness: float\n",
    "    specificity: float\n",
    "    completeness: float\n",
    "    evidence_strength: float\n",
    "    inconsistency_flag: bool\n",
    "    notes: Optional[str]\n",
    "\n",
    "\n",
    "class QA(TypedDict):\n",
    "    skill: str\n",
    "    q_id: str\n",
    "    question: str\n",
    "    answer: str\n",
    "    eval: EvalJSON\n",
    "    reward: float\n",
    "\n",
    "\n",
    "class SkillStat(TypedDict):\n",
    "    n: int\n",
    "    mean: float\n",
    "    M2: float\n",
    "    se: float\n",
    "    lcb: float\n",
    "    status: Status\n",
    "\n",
    "\n",
    "class InterviewState(TypedDict):\n",
    "    turn: int\n",
    "    turn_budget: int\n",
    "    skills: List[str]\n",
    "    claims: Dict[str, List[Dict]]\n",
    "    stats: Dict[str, SkillStat]\n",
    "    last_qa: List[QA]\n",
    "    candidates: List[Dict]\n",
    "    selected_q: Optional[Dict]\n",
    "    interview_complete: bool\n",
    "    summary: Optional[Dict]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "401fd100",
   "metadata": {},
   "outputs": [],
   "source": [
    "def welford_update(s: SkillStat, r: float) -> SkillStat:\n",
    "    n = s[\"n\"] + 1\n",
    "    delta = r - s[\"mean\"]\n",
    "    mean = s[\"mean\"] + delta / n\n",
    "    M2 = s[\"M2\"] + delta * (r - mean)\n",
    "    var = (M2 / (n - 1)) if n > 1 else 0.25  # conservative early variance\n",
    "    se = math.sqrt(var / max(1, n))\n",
    "    z = 1.64  # ~95% one-sided\n",
    "    lcb = mean - z * se\n",
    "    status: Status = (\n",
    "        \"verified\"\n",
    "        if (n >= 3 and lcb >= 0.75)\n",
    "        else \"probed\"\n",
    "        if (mean >= 0.6 or n >= 2)\n",
    "        else \"unverified\"\n",
    "    )\n",
    "    return {\"n\": n, \"mean\": mean, \"M2\": M2, \"se\": se, \"lcb\": lcb, \"status\": status}\n",
    "\n",
    "\n",
    "def blend_reward(ev: EvalJSON) -> float:\n",
    "    r = (\n",
    "        0.35 * ev[\"correctness\"]\n",
    "        + 0.25 * ev[\"specificity\"]\n",
    "        + 0.10 * ev[\"completeness\"]\n",
    "        + 0.20 * ev[\"evidence_strength\"]\n",
    "        - (0.30 if ev[\"inconsistency_flag\"] else 0.0)\n",
    "    )\n",
    "    return max(0.0, min(1.0, r))\n",
    "\n",
    "\n",
    "def need_to_stop(state: InterviewState) -> bool:\n",
    "    verified = sum(1 for s in state[\"stats\"].values() if s[\"status\"] == \"verified\")\n",
    "    return (verified >= 2) or (state[\"turn\"] >= state[\"turn_budget\"])\n",
    "\n",
    "\n",
    "def summarize_stat(s: SkillStat) -> str:\n",
    "    return f\"n={s['n']} mean={s['mean']:.2f} lcb={s['lcb']:.2f} se={s['se']:.2f} status={s['status']}\"\n",
    "\n",
    "\n",
    "def policy_score(skill_stat: SkillStat, early: bool) -> float:\n",
    "    if early:\n",
    "        return skill_stat[\"se\"] + 0.3 * (1 - skill_stat[\"mean\"])\n",
    "    near_bonus = (\n",
    "        0.15\n",
    "        if (\n",
    "            skill_stat[\"mean\"] >= 0.70\n",
    "            and skill_stat[\"lcb\"] < 0.75\n",
    "            and skill_stat[\"status\"] != \"verified\"\n",
    "        )\n",
    "        else 0.0\n",
    "    )\n",
    "    return skill_stat[\"mean\"] - 0.2 * skill_stat[\"se\"] + near_bonus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "af87d038",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mock_generate_candidates(state: InterviewState) -> List[Dict]:\n",
    "    # Produce simple but sensible question candidates\n",
    "    rng = random.Random(1337 + state[\"turn\"])\n",
    "    out = []\n",
    "    for skill in state[\"skills\"]:\n",
    "        if len(out) >= 5:\n",
    "            break\n",
    "        diff = rng.choice([1, 2, 2, 3])\n",
    "        probe = rng.choice([\"how\", \"why\", \"design\", \"debug\", \"critique\", \"code\"])\n",
    "        qtext = {\n",
    "            \"PyTorch\": \"Show a minimal training loop and explain how you manage optimizer and scheduler.\",\n",
    "            \"LLM Evaluation\": \"Design a small test to detect spec gaming; what metrics and guardrails would you use?\",\n",
    "            \"CUDA\": \"Explain how to diagnose an out-of-memory error and what tooling you would check first.\",\n",
    "        }.get(\n",
    "            skill,\n",
    "            f\"Give me one concrete, falsifiable example proving your competence in {skill}.\",\n",
    "        )\n",
    "        out.append(\n",
    "            {\n",
    "                \"id\": f\"q_{skill}_{state['turn']}_{diff}\",\n",
    "                \"skill\": skill,\n",
    "                \"difficulty\": diff,\n",
    "                \"probe_type\": probe,\n",
    "                \"text\": qtext,\n",
    "                \"expected_markers\": {\n",
    "                    \"PyTorch\": [\n",
    "                        \"DataLoader\",\n",
    "                        \"optimizer\",\n",
    "                        \"scheduler\",\n",
    "                        \"backward\",\n",
    "                        \"zero_grad\",\n",
    "                        \"AdamW\",\n",
    "                        \"param_groups\",\n",
    "                    ],\n",
    "                    \"LLM Evaluation\": [\n",
    "                        \"gold data\",\n",
    "                        \"leakage\",\n",
    "                        \"inter-annotator\",\n",
    "                        \"specification gaming\",\n",
    "                        \"ROC/AUC\",\n",
    "                    ],\n",
    "                    \"CUDA\": [\n",
    "                        \"nvidia-smi\",\n",
    "                        \"OOM\",\n",
    "                        \"profiling\",\n",
    "                        \"torch.cuda.memory\",\n",
    "                        \"mixed precision\",\n",
    "                    ],\n",
    "                }.get(skill, []),\n",
    "            }\n",
    "        )\n",
    "    return out\n",
    "\n",
    "\n",
    "def mock_simulate_answer(q: Dict) -> str:\n",
    "    # Deterministic, mildly skillful canned answers\n",
    "    if q[\"skill\"] == \"PyTorch\":\n",
    "        return \"Use DataLoader, model.train(); optimizer=AdamW with param_groups to exclude bias/LayerNorm from weight_decay; zero_grad(); loss.backward(); optimizer.step(); scheduler.step(); watch exploding grads.\"\n",
    "    if q[\"skill\"] == \"LLM Evaluation\":\n",
    "        return \"Use a held-out gold set; check leakage; track inter-annotator agreement; watch spec gaming by adding adversarial prompts; report precision/recall and ROC; add manual spot checks.\"\n",
    "    if q[\"skill\"] == \"CUDA\":\n",
    "        return \"Start with nvidia-smi, then torch.cuda.memory_summary(); try smaller batch, AMP; profile with Nsight; check tensor shapes; fix dataloader pin_memory; monitor fragmentation.\"\n",
    "    return \"I would provide a concrete example from past work with tools, logs, and a minimal repro.\"\n",
    "\n",
    "\n",
    "def mock_grade_answer(qa: Dict, state: InterviewState) -> EvalJSON:\n",
    "    # Grade based on overlap with expected_markers and some noise\n",
    "    markers = qa.get(\"expected_markers\", [])\n",
    "    ans = qa[\"answer\"].lower()\n",
    "    hit = sum(1 for m in markers if m.lower() in ans)\n",
    "    k = len(markers) or 1\n",
    "    specificity = min(1.0, 0.4 + 0.6 * (hit / k))\n",
    "    correctness = min(1.0, 0.6 + 0.4 * (hit / k))\n",
    "    completeness = 0.6 if hit / k >= 0.5 else 0.4\n",
    "    evidence_strength = (\n",
    "        0.6\n",
    "        if any(x in ans for x in [\"repo\", \"code\", \"log\", \"nsight\", \"param_groups\"])\n",
    "        else 0.45\n",
    "    )\n",
    "    inconsistency_flag = False  # deterministic MOCK\n",
    "    return {\n",
    "        \"correctness\": round(correctness, 2),\n",
    "        \"specificity\": round(specificity, 2),\n",
    "        \"completeness\": round(completeness, 2),\n",
    "        \"evidence_strength\": round(evidence_strength, 2),\n",
    "        \"inconsistency_flag\": inconsistency_flag,\n",
    "        \"notes\": \"MOCK grading\",\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8bc70a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm_generate_questions(state: InterviewState) -> List[Dict]:\n",
    "    if MOCK:\n",
    "        return mock_generate_candidates(state)\n",
    "\n",
    "    from langgraph.graph import StateGraph\n",
    "    from langchain_openai import ChatOpenAI\n",
    "    from langchain.output_parsers import JsonOutputParser\n",
    "    from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "    llm = ChatOpenAI(model=\"gpt-4\", temperature=0.1)\n",
    "\n",
    "    template = \"\"\"You are an expert technical interviewer. Generate 3 questions to verify the candidate's expertise in their claimed skills.\n",
    "\n",
    "Current interview state:\n",
    "- Skills being assessed: {skills}\n",
    "- Previous Q&A history: {last_qa}\n",
    "- Current skill statistics: {stats} \n",
    "- Remaining turns: {remaining_turns}\n",
    "\n",
    "Generate questions that:\n",
    "1. Focus on skills with high uncertainty (high standard error)\n",
    "2. Vary difficulty based on previous answers\n",
    "3. Are concrete and falsifiable\n",
    "4. Target specific technical knowledge\n",
    "\n",
    "Return a list of question objects in the following format:\n",
    "[{{\"id\": \"q_[skill]_[turn]_[difficulty]\", \n",
    "   \"skill\": \"[skill name]\",\n",
    "   \"difficulty\": \"[easy/medium/hard]\",\n",
    "   \"probe_type\": \"[concept/application/debugging]\",\n",
    "   \"text\": \"[question text]\",\n",
    "   \"expected_markers\": [\"[key terms/concepts that should appear in good answers]\"]\n",
    "}}]\"\"\"\n",
    "\n",
    "    prompt = ChatPromptTemplate.from_template(template)\n",
    "    parser = JsonOutputParser()\n",
    "\n",
    "    # Define the question generation node\n",
    "    def generate_node(state):\n",
    "        chain = prompt | llm | parser\n",
    "        questions = chain.invoke(\n",
    "            {\n",
    "                \"skills\": state[\"skills\"],\n",
    "                \"last_qa\": state[\"last_qa\"],\n",
    "                \"stats\": state[\"stats\"],\n",
    "                \"remaining_turns\": state[\"turn_budget\"] - state[\"turn\"],\n",
    "            }\n",
    "        )\n",
    "        return {\"questions\": questions}\n",
    "\n",
    "    # Create graph\n",
    "    workflow = StateGraph(nodes=[])\n",
    "\n",
    "    # Add node\n",
    "    workflow.add_node(\"generate\", generate_node)\n",
    "    workflow.set_entry_point(\"generate\")\n",
    "    workflow.set_finish_point(\"generate\")\n",
    "\n",
    "    # Compile and run\n",
    "    app = workflow.compile()\n",
    "    result = app.invoke({\"state\": state})\n",
    "\n",
    "    return result[\"questions\"]\n",
    "\n",
    "\n",
    "def llm_grade_answer(qa: Dict, state: InterviewState) -> EvalJSON:\n",
    "    if MOCK:\n",
    "        return mock_grade_answer(qa, state)\n",
    "\n",
    "    from langgraph.graph import StateGraph\n",
    "    from langchain_openai import ChatOpenAI\n",
    "    from langchain.output_parsers import JsonOutputParser\n",
    "    from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "    llm = ChatOpenAI(model=\"gpt-4\", temperature=0.1)\n",
    "\n",
    "    template = \"\"\"Grade this technical interview response.\n",
    "\n",
    "Question: {question}\n",
    "Answer: {answer}\n",
    "\n",
    "Expected key concepts/markers: {markers}\n",
    "\n",
    "Evaluate the response on:\n",
    "1. Correctness (0-1): Technical accuracy and proper use of concepts\n",
    "2. Specificity (0-1): Level of detail and precision\n",
    "3. Completeness (0-1): Coverage of relevant aspects\n",
    "4. Evidence strength (0-1): Concrete examples/experience demonstrated\n",
    "5. Check for inconsistencies with technical best practices\n",
    "\n",
    "Return evaluation in this format:\n",
    "{{\n",
    "    \"correctness\": float,\n",
    "    \"specificity\": float, \n",
    "    \"completeness\": float,\n",
    "    \"evidence_strength\": float,\n",
    "    \"inconsistency_flag\": boolean,\n",
    "    \"notes\": \"Brief justification\"\n",
    "}}\"\"\"\n",
    "\n",
    "    prompt = ChatPromptTemplate.from_template(template)\n",
    "    parser = JsonOutputParser()\n",
    "\n",
    "    # Define the grading node\n",
    "    def grade_node(state):\n",
    "        chain = prompt | llm | parser\n",
    "        evaluation = chain.invoke(\n",
    "            {\n",
    "                \"question\": qa[\"question\"],\n",
    "                \"answer\": qa[\"answer\"],\n",
    "                \"markers\": qa.get(\"expected_markers\", []),\n",
    "            }\n",
    "        )\n",
    "        return {\"evaluation\": evaluation}\n",
    "\n",
    "    # Create graph\n",
    "    workflow = StateGraph(nodes=[])\n",
    "\n",
    "    # Add node\n",
    "    workflow.add_node(\"grade\", grade_node)\n",
    "    workflow.set_entry_point(\"grade\")\n",
    "    workflow.set_finish_point(\"grade\")\n",
    "\n",
    "    # Compile and run\n",
    "    app = workflow.compile()\n",
    "    result = app.invoke({\"state\": {\"qa\": qa}})\n",
    "\n",
    "    return result[\"evaluation\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "fa1b7b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_from_claims(state: InterviewState) -> InterviewState:\n",
    "    if state[\"turn\"] > 0:\n",
    "        return state\n",
    "    stats = {}\n",
    "    for skill in state[\"skills\"]:\n",
    "        stats[skill] = {\n",
    "            \"n\": 0,\n",
    "            \"mean\": 0.5,\n",
    "            \"M2\": 0.0,\n",
    "            \"se\": 0.5,\n",
    "            \"lcb\": 0.0,\n",
    "            \"status\": \"unverified\",\n",
    "        }  # neutral start\n",
    "    state[\"stats\"] = stats\n",
    "    state[\"last_qa\"] = []\n",
    "    state[\"candidates\"] = []\n",
    "    state[\"selected_q\"] = None\n",
    "    state[\"interview_complete\"] = False\n",
    "    return state\n",
    "\n",
    "\n",
    "def generate_candidates(state: InterviewState) -> InterviewState:\n",
    "    state[\"candidates\"] = llm_generate_questions(state)\n",
    "    return state\n",
    "\n",
    "\n",
    "def select_question(state: InterviewState) -> InterviewState:\n",
    "    t, T = state[\"turn\"], state[\"turn_budget\"]\n",
    "    early = t < (T // 2)\n",
    "    scored = [\n",
    "        (policy_score(state[\"stats\"][q[\"skill\"]], early), q)\n",
    "        for q in state[\"candidates\"]\n",
    "    ]\n",
    "    state[\"selected_q\"] = max(scored, key=lambda x: x[0])[1]\n",
    "    return state\n",
    "\n",
    "\n",
    "def ask_and_collect(state: InterviewState) -> InterviewState:\n",
    "    q = state[\"selected_q\"]\n",
    "    # In a real app, you'd interrupt here and collect human input\n",
    "    answer = (\n",
    "        mock_simulate_answer(q) if MOCK else \"\"\n",
    "    )  # replace with interrupt in real flow\n",
    "    qa = {\n",
    "        \"skill\": q[\"skill\"],\n",
    "        \"q_id\": q[\"id\"],\n",
    "        \"question\": q[\"text\"],\n",
    "        \"answer\": answer,\n",
    "        \"eval\": None,\n",
    "        \"reward\": 0.0,\n",
    "        \"expected_markers\": q.get(\"expected_markers\", []),\n",
    "    }\n",
    "    state[\"last_qa\"] = (state[\"last_qa\"] + [qa])[-3:]\n",
    "    return state\n",
    "\n",
    "\n",
    "def grade_answer(state: InterviewState) -> InterviewState:\n",
    "    qa = state[\"last_qa\"][-1]\n",
    "    # Include markers in the call for grading\n",
    "    qa_for_grade = {\n",
    "        \"question\": qa[\"question\"],\n",
    "        \"answer\": qa[\"answer\"],\n",
    "        \"expected_markers\": qa.get(\"expected_markers\", []),\n",
    "    }\n",
    "    ev = llm_grade_answer(qa_for_grade, state)\n",
    "    qa[\"eval\"] = ev\n",
    "    qa[\"reward\"] = blend_reward(ev)\n",
    "    state[\"last_qa\"][-1] = qa\n",
    "    return state\n",
    "\n",
    "\n",
    "def update_belief(state: InterviewState) -> InterviewState:\n",
    "    qa = state[\"last_qa\"][-1]\n",
    "    sk = qa[\"skill\"]\n",
    "    state[\"stats\"][sk] = welford_update(state[\"stats\"][sk], qa[\"reward\"])\n",
    "    state[\"turn\"] += 1\n",
    "    return state\n",
    "\n",
    "\n",
    "def decide_next(state: InterviewState):\n",
    "    return \"finalise\" if need_to_stop(state) else \"loop\"\n",
    "\n",
    "\n",
    "def finalise_report(state: InterviewState) -> InterviewState:\n",
    "    cards = []\n",
    "    for skill, s in state[\"stats\"].items():\n",
    "        cards.append(\n",
    "            {\n",
    "                \"skill\": skill,\n",
    "                \"status\": s[\"status\"],\n",
    "                \"score_mean\": round(s[\"mean\"], 2),\n",
    "                \"lcb\": round(s[\"lcb\"], 2),\n",
    "                \"n\": s[\"n\"],\n",
    "                \"notes\": f\"se={s['se']:.2f}\",\n",
    "            }\n",
    "        )\n",
    "    state[\"summary\"] = {\"verification_cards\": cards}\n",
    "    state[\"interview_complete\"] = True\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8936f649",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = StateGraph(InterviewState)\n",
    "graph.add_node(\"init\", init_from_claims)\n",
    "graph.add_node(\"generate_candidates\", generate_candidates)\n",
    "graph.add_node(\"select_question\", select_question)\n",
    "graph.add_node(\"ask_and_collect\", ask_and_collect)\n",
    "graph.add_node(\"grade_answer\", grade_answer)\n",
    "graph.add_node(\"update_belief\", update_belief)\n",
    "graph.add_node(\"finalise\", finalise_report)\n",
    "\n",
    "graph.set_entry_point(\"init\")\n",
    "graph.add_edge(\"init\", \"generate_candidates\")\n",
    "graph.add_edge(\"generate_candidates\", \"select_question\")\n",
    "graph.add_edge(\"select_question\", \"ask_and_collect\")\n",
    "graph.add_edge(\"ask_and_collect\", \"grade_answer\")\n",
    "graph.add_edge(\"grade_answer\", \"update_belief\")\n",
    "graph.add_conditional_edges(\n",
    "    \"update_belief\",\n",
    "    decide_next,\n",
    "    {\"loop\": \"generate_candidates\", \"finalise\": \"finalise\"},\n",
    ")\n",
    "\n",
    "app = graph.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9939f18b",
   "metadata": {},
   "source": [
    "\n",
    "## 1) Demo run (MOCK mode)\n",
    "\n",
    "We simulate answers and grading. You’ll see **turn-by-turn** stats and a final **Verification Card** table.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d033e809",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'JsonOutputParser' from 'langchain.output_parsers' (/Users/jason_macstudio/.venv/lib/python3.11/site-packages/langchain/output_parsers/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[42]\u001b[39m\u001b[32m, line 19\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Initial state with 3 claimed skills and a small turn budget\u001b[39;00m\n\u001b[32m      2\u001b[39m initial_state: InterviewState = {\n\u001b[32m      3\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mturn\u001b[39m\u001b[33m\"\u001b[39m: \u001b[32m0\u001b[39m,\n\u001b[32m      4\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mturn_budget\u001b[39m\u001b[33m\"\u001b[39m: \u001b[32m8\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     16\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33msummary\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m     17\u001b[39m }\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m state = \u001b[43mapp\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43minitial_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[38;5;66;03m# Pretty print turn-by-turn evolution from the mock last_qa (only last 3 saved)\u001b[39;00m\n\u001b[32m     22\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mFinal per-skill stats:\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.venv/lib/python3.11/site-packages/langgraph/pregel/main.py:3085\u001b[39m, in \u001b[36mPregel.invoke\u001b[39m\u001b[34m(self, input, config, context, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, durability, **kwargs)\u001b[39m\n\u001b[32m   3082\u001b[39m chunks: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any] | Any] = []\n\u001b[32m   3083\u001b[39m interrupts: \u001b[38;5;28mlist\u001b[39m[Interrupt] = []\n\u001b[32m-> \u001b[39m\u001b[32m3085\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3086\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   3087\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3088\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3089\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mupdates\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mvalues\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m   3090\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mvalues\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m   3091\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3092\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprint_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprint_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3093\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_keys\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3094\u001b[39m \u001b[43m    \u001b[49m\u001b[43minterrupt_before\u001b[49m\u001b[43m=\u001b[49m\u001b[43minterrupt_before\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3095\u001b[39m \u001b[43m    \u001b[49m\u001b[43minterrupt_after\u001b[49m\u001b[43m=\u001b[49m\u001b[43minterrupt_after\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3096\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdurability\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdurability\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3097\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3098\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   3099\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mvalues\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\n\u001b[32m   3100\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m:\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.venv/lib/python3.11/site-packages/langgraph/pregel/main.py:2674\u001b[39m, in \u001b[36mPregel.stream\u001b[39m\u001b[34m(self, input, config, context, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, durability, subgraphs, debug, **kwargs)\u001b[39m\n\u001b[32m   2672\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m task \u001b[38;5;129;01min\u001b[39;00m loop.match_cached_writes():\n\u001b[32m   2673\u001b[39m     loop.output_writes(task.id, task.writes, cached=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m-> \u001b[39m\u001b[32m2674\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrunner\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtick\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2675\u001b[39m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mloop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtasks\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwrites\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2676\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstep_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2677\u001b[39m \u001b[43m    \u001b[49m\u001b[43mget_waiter\u001b[49m\u001b[43m=\u001b[49m\u001b[43mget_waiter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2678\u001b[39m \u001b[43m    \u001b[49m\u001b[43mschedule_task\u001b[49m\u001b[43m=\u001b[49m\u001b[43mloop\u001b[49m\u001b[43m.\u001b[49m\u001b[43maccept_push\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2679\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   2680\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# emit output\u001b[39;49;00m\n\u001b[32m   2681\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01myield from\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_output\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2682\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprint_mode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msubgraphs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mqueue\u001b[49m\u001b[43m.\u001b[49m\u001b[43mEmpty\u001b[49m\n\u001b[32m   2683\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2684\u001b[39m loop.after_tick()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.venv/lib/python3.11/site-packages/langgraph/pregel/_runner.py:162\u001b[39m, in \u001b[36mPregelRunner.tick\u001b[39m\u001b[34m(self, tasks, reraise, timeout, retry_policy, get_waiter, schedule_task)\u001b[39m\n\u001b[32m    160\u001b[39m t = tasks[\u001b[32m0\u001b[39m]\n\u001b[32m    161\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m162\u001b[39m     \u001b[43mrun_with_retry\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    163\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    164\u001b[39m \u001b[43m        \u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    165\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconfigurable\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m    166\u001b[39m \u001b[43m            \u001b[49m\u001b[43mCONFIG_KEY_CALL\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpartial\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    167\u001b[39m \u001b[43m                \u001b[49m\u001b[43m_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    168\u001b[39m \u001b[43m                \u001b[49m\u001b[43mweakref\u001b[49m\u001b[43m.\u001b[49m\u001b[43mref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    169\u001b[39m \u001b[43m                \u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    170\u001b[39m \u001b[43m                \u001b[49m\u001b[43mfutures\u001b[49m\u001b[43m=\u001b[49m\u001b[43mweakref\u001b[49m\u001b[43m.\u001b[49m\u001b[43mref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfutures\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    171\u001b[39m \u001b[43m                \u001b[49m\u001b[43mschedule_task\u001b[49m\u001b[43m=\u001b[49m\u001b[43mschedule_task\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    172\u001b[39m \u001b[43m                \u001b[49m\u001b[43msubmit\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msubmit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    173\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    174\u001b[39m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    175\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    176\u001b[39m     \u001b[38;5;28mself\u001b[39m.commit(t, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    177\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.venv/lib/python3.11/site-packages/langgraph/pregel/_retry.py:42\u001b[39m, in \u001b[36mrun_with_retry\u001b[39m\u001b[34m(task, retry_policy, configurable)\u001b[39m\n\u001b[32m     40\u001b[39m     task.writes.clear()\n\u001b[32m     41\u001b[39m     \u001b[38;5;66;03m# run the task\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m42\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtask\u001b[49m\u001b[43m.\u001b[49m\u001b[43mproc\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m.\u001b[49m\u001b[43minput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     43\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ParentCommand \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m     44\u001b[39m     ns: \u001b[38;5;28mstr\u001b[39m = config[CONF][CONFIG_KEY_CHECKPOINT_NS]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.venv/lib/python3.11/site-packages/langgraph/_internal/_runnable.py:657\u001b[39m, in \u001b[36mRunnableSeq.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    655\u001b[39m     \u001b[38;5;66;03m# run in context\u001b[39;00m\n\u001b[32m    656\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m set_config_context(config, run) \u001b[38;5;28;01mas\u001b[39;00m context:\n\u001b[32m--> \u001b[39m\u001b[32m657\u001b[39m         \u001b[38;5;28minput\u001b[39m = \u001b[43mcontext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    658\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    659\u001b[39m     \u001b[38;5;28minput\u001b[39m = step.invoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.venv/lib/python3.11/site-packages/langgraph/_internal/_runnable.py:401\u001b[39m, in \u001b[36mRunnableCallable.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    399\u001b[39m         run_manager.on_chain_end(ret)\n\u001b[32m    400\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m401\u001b[39m     ret = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    402\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.recurse \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ret, Runnable):\n\u001b[32m    403\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m ret.invoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[40]\u001b[39m\u001b[32m, line 15\u001b[39m, in \u001b[36mgenerate_candidates\u001b[39m\u001b[34m(state)\u001b[39m\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mgenerate_candidates\u001b[39m(state: InterviewState) -> InterviewState:\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m     state[\u001b[33m\"\u001b[39m\u001b[33mcandidates\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[43mllm_generate_questions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     16\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m state\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[39]\u001b[39m\u001b[32m, line 7\u001b[39m, in \u001b[36mllm_generate_questions\u001b[39m\u001b[34m(state)\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlanggraph\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mgraph\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m StateGraph\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_openai\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ChatOpenAI\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01moutput_parsers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m JsonOutputParser\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mprompts\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ChatPromptTemplate\n\u001b[32m     10\u001b[39m llm = ChatOpenAI(model=\u001b[33m\"\u001b[39m\u001b[33mgpt-4\u001b[39m\u001b[33m\"\u001b[39m, temperature=\u001b[32m0.1\u001b[39m)\n",
      "\u001b[31mImportError\u001b[39m: cannot import name 'JsonOutputParser' from 'langchain.output_parsers' (/Users/jason_macstudio/.venv/lib/python3.11/site-packages/langchain/output_parsers/__init__.py)",
      "During task with name 'generate_candidates' and id '2e637af5-e233-23d4-559b-b86f17557fd1'"
     ]
    }
   ],
   "source": [
    "# Initial state with 3 claimed skills and a small turn budget\n",
    "initial_state: InterviewState = {\n",
    "    \"turn\": 0,\n",
    "    \"turn_budget\": 8,\n",
    "    \"skills\": [\"PyTorch\", \"LLM Evaluation\", \"CUDA\"],\n",
    "    \"claims\": {\n",
    "        \"PyTorch\": [{\"src\": \"CV\", \"confidence\": 0.7}],\n",
    "        \"LLM Evaluation\": [{\"src\": \"LinkedIn\", \"confidence\": 0.6}],\n",
    "        \"CUDA\": [{\"src\": \"CV\", \"confidence\": 0.5}],\n",
    "    },\n",
    "    \"stats\": {},\n",
    "    \"last_qa\": [],\n",
    "    \"candidates\": [],\n",
    "    \"selected_q\": None,\n",
    "    \"interview_complete\": False,\n",
    "    \"summary\": None,\n",
    "}\n",
    "\n",
    "state = app.invoke(initial_state)\n",
    "\n",
    "# Pretty print turn-by-turn evolution from the mock last_qa (only last 3 saved)\n",
    "print(\"Final per-skill stats:\")\n",
    "for skill, st in state[\"stats\"].items():\n",
    "    print(f\"- {skill}: {summarize_stat(st)}\")\n",
    "\n",
    "print(\"\\nVerification Cards:\")\n",
    "print(json.dumps(state[\"summary\"], indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09528a8b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
